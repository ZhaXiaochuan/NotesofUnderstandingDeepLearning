{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.modules):\n",
    "    # init\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1939, 0.0715, 0.1348, 0.0252, 0.1160, 0.1652, 0.0224, 0.0188, 0.0585,\n",
      "         0.1936]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def Basic_Attention(x_m, x_n):\n",
    "    '''The simple implementation of self-attention. It is for x_n. We need all the values of x_m (m:1...N).\n",
    "    Let's assume the dim is 1024, N is 10. So, the input shape is 10*1024, 1*1024. Output shape is 1*10.\n",
    "    '''\n",
    "    dim = 1024\n",
    "    N = 10\n",
    "\n",
    "    get_key = nn.Linear(1024, 1024)\n",
    "    get_query = nn.Linear(1024, 1024)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    key_m = get_key(x_m)\n",
    "    query_n = get_query(x_n)\n",
    "    atten = torch.matmul(key_m ,torch.transpose(query_n, 0 ,1)).reshape(1,N)\n",
    "    return softmax(atten)\n",
    "\n",
    "x_m = torch.rand(10,1024)\n",
    "x_n = torch.rand(1,1024)\n",
    "attention = Basic_Attention(x_m, x_n)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Attention(x):\n",
    "    ''' An intergate version of self-attention. \n",
    "        The input is 10*1024. The output is the combation of each attention value of x_m and x_n like Basic Attention.\n",
    "        An effective implementation is use matrix, instead of loop.\n",
    "    '''\n",
    "    get_key = nn.Linear(1024,1024)\n",
    "    get_query = nn.Linear(1024,1024)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    keys = get_key(x)\n",
    "    queries = get_query(x)\n",
    "    atten = torch.matmul(keys, torch.transpose(queries, 0 ,1)).reshape(10,10)\n",
    "    return softmax(atten)\n",
    "\n",
    "x = torch.rand(10,1024)\n",
    "a = Attention(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4150,  0.4764,  0.4953,  ...,  0.9375,  0.0478,  1.0444],\n",
       "        [ 0.5621, -0.0397,  0.5774,  ...,  0.2467,  0.4555,  0.6840],\n",
       "        [ 0.2972,  0.1832,  0.6594,  ...,  0.5590,  0.6558,  0.3696],\n",
       "        ...,\n",
       "        [ 0.3009, -0.0174,  0.9722,  ...,  0.1205,  0.3482,  0.5563],\n",
       "        [ 0.6826, -0.1424,  0.4323,  ...,  0.1914,  0.7104,  0.2910],\n",
       "        [ 1.0925,  0.6171,  0.2626,  ...,  0.4824,  0.7947,  0.8305]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer(x):\n",
    "    '''transformer is more complex, including self-attention and update'''\n",
    "    attention = Attention(x)\n",
    "    get_value = nn.Linear(1024,1024)\n",
    "    \n",
    "    values = get_value(x)\n",
    "    return torch.matmul(attention, values)\n",
    "x = torch.rand(10,1024)\n",
    "t = transformer(x)\n",
    "x-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 10])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Linear(10,10)\n",
    "a = torch.rand(1024,10)\n",
    "l(a).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
